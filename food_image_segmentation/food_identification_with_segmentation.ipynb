{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RLuUdh9MAHIG",
    "outputId": "d64033cb-de1d-4456-f235-9736130d6d55"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "w3MKdOdAAUJb",
    "outputId": "653a75f9-c0e5-47d7-d986-878afe4c779c"
   },
   "outputs": [],
   "source": [
    "!pip install torchmetrics\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torchvision.models.segmentation import deeplabv3_resnet101\n",
    "\n",
    "from torchvision.transforms import functional as F\n",
    "from torchmetrics.classification import JaccardIndex\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageFilter\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Setting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTY_20gOvOkT"
   },
   "source": [
    "### Unzipping Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R8nQlXhWA97R",
    "outputId": "10a66d2a-b677-4524-e4a7-3b1ae47fd7f5"
   },
   "outputs": [],
   "source": [
    "os.makedirs(\"/content/advanced_ai/task_1/food\", exist_ok=True)\n",
    "!unzip '/content/drive/MyDrive/advanced_ai/task1/Task 1 Food Segmentation Dataset.zip' -d /content/advanced_ai/task_1/food"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bxt9IPtRvVwL"
   },
   "source": [
    "#### Creating separate folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cpf0I7D7SEIu"
   },
   "outputs": [],
   "source": [
    "# Directories for masks (annotations) and images\n",
    "annot_train = \"/content/advanced_ai/task_1/food/Task 1 Food Segmentation Dataset/FoodSeg103/Images/ann_dir/train\"\n",
    "annot_test = \"/content/advanced_ai/task_1/food/Task 1 Food Segmentation Dataset/FoodSeg103/Images/ann_dir/test\"\n",
    "image_train = \"/content/advanced_ai/task_1/food/Task 1 Food Segmentation Dataset/FoodSeg103/Images/img_dir/train\"\n",
    "image_test = \"/content/advanced_ai/task_1/food/Task 1 Food Segmentation Dataset/FoodSeg103/Images/img_dir/test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XpYmg8_cIGcW"
   },
   "source": [
    "## Loading Images and Masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5oJYPwJ1zfHW"
   },
   "outputs": [],
   "source": [
    "test_image_name = \"00006149.jpg\"\n",
    "\n",
    "def filepaths(directory):\n",
    "    files = os.listdir(directory)\n",
    "    files = sorted(files)\n",
    "    filepaths = [os.path.join(directory, f) for f in files if f.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    return filepaths\n",
    "\n",
    "# Lists of filepaths for training\n",
    "train_image_paths = filepaths(image_train)\n",
    "train_mask_paths = filepaths(annot_train)\n",
    "\n",
    "# List of filepath for test image and it's mask\n",
    "test_image_paths = [os.path.join(image_test, test_image_name)]\n",
    "test_mask_paths = [os.path.join(annot_test, test_image_name.replace(\".jpg\", \".png\"))]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOZvoDopIPtj"
   },
   "source": [
    "## Image Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ywa8mcWeH_JP"
   },
   "outputs": [],
   "source": [
    "def training_augmentations(image, mask, image_size=(256, 256)):\n",
    "    # Resizing both the images and masks\n",
    "    image = F.resize(image, image_size)\n",
    "    mask = F.resize(mask, image_size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "    # Random horizontal flip\n",
    "    if random.random() > 0.5:\n",
    "        image = F.hflip(image)\n",
    "        mask = F.hflip(mask)\n",
    "\n",
    "    # Random rotation\n",
    "    angle = random.uniform(-15, 15)\n",
    "    image = F.rotate(image, angle)\n",
    "    mask = F.rotate(mask, angle)\n",
    "\n",
    "    # Random affine transformation (translation and slight rotation)\n",
    "    affine_params = transforms.RandomAffine.get_params(\n",
    "        degrees=(-10, 10),\n",
    "        translate=(0.05, 0.05),\n",
    "        scale_ranges=None,\n",
    "        shears=None,\n",
    "        img_size=image.size\n",
    "    )\n",
    "    image = F.affine(image, *affine_params)\n",
    "    mask = F.affine(mask, *affine_params)\n",
    "\n",
    "    # Applying color jitter and gaussian blur - This is only for images and not masks\n",
    "    image = F.adjust_brightness(image, random.uniform(0.8, 1.2))\n",
    "    image = F.adjust_contrast(image, random.uniform(0.8, 1.2))\n",
    "    image = F.adjust_saturation(image, random.uniform(0.8, 1.2))\n",
    "    image = F.adjust_hue(image, random.uniform(-0.1, 0.1))\n",
    "    image = F.gaussian_blur(image, kernel_size=3, sigma=random.uniform(0.1, 2.0))\n",
    "\n",
    "    # Converting images to tensor and normalizing them; converting masks to tensors - Keeping the labels as is\n",
    "    image = F.to_tensor(image)\n",
    "    image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    mask = torch.tensor(np.array(mask), dtype=torch.long)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "def test_augmentations(image, mask, image_size=(256, 256)):\n",
    "    # For test data, we only apply resize and normalization, no other augmentations\n",
    "    image = F.resize(image, image_size)\n",
    "    mask = F.resize(mask, image_size, interpolation=transforms.InterpolationMode.NEAREST)\n",
    "\n",
    "    image = F.to_tensor(image)\n",
    "    image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    mask = torch.tensor(np.array(mask), dtype=torch.long)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "# Creating dataset using the augmentation functions\n",
    "def dataset_for_segmentation(image_paths, mask_paths, augmentation_function):\n",
    "    images = []\n",
    "    masks = []\n",
    "    for img_path, mask_path in zip(image_paths, mask_paths):\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "        aug_image, aug_mask = augmentation_function(image, mask)\n",
    "        images.append(aug_image)\n",
    "        masks.append(aug_mask)\n",
    "    return images, masks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRSvAGyeIS0f"
   },
   "source": [
    "## Creating Data Loaders for PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3yD53bLxUSOH",
    "outputId": "ad031827-a76e-4354-95d4-8ce0ac469ca7"
   },
   "outputs": [],
   "source": [
    "# Loading the data with the augmentation functions\n",
    "train_images, train_masks = dataset_for_segmentation(train_image_paths, train_mask_paths, training_augmentations)\n",
    "test_images, test_masks = dataset_for_segmentation(test_image_paths, test_mask_paths, test_augmentations)\n",
    "\n",
    "# Converting lists to DataLoaders\n",
    "def create_data_loader(image_data, mask_data, batch_size=4, shuffle=True):\n",
    "    dataset = list(zip(image_data, mask_data))\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "train_loader = create_data_loader(train_images, train_masks, batch_size=4, shuffle=True)\n",
    "test_loader = create_data_loader(test_images, test_masks, batch_size=4, shuffle=False)\n",
    "\n",
    "# Checking the batch shapes for the 1st batch only!\n",
    "for imgs, masks in train_loader:\n",
    "    print(\"Image batch shape:\", imgs.shape)  # [batch_size, 3, 256, 256]\n",
    "    print(\"Mask batch shape:\", masks.shape)    # [batch_size, 256, 256]\n",
    "    print(\"Unique values in first mask batch:\", torch.unique(masks))\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nEbBK9BevUFh"
   },
   "source": [
    "## Sample images from the train dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6l08cbHaYMwF",
    "outputId": "6ff8728a-6fce-4065-d900-f283e9384b15"
   },
   "outputs": [],
   "source": [
    "# Selecting a batch from the train_loader\n",
    "imgs, masks = next(iter(train_loader))\n",
    "\n",
    "# Converting tensors to NumPy arrays for visualization\n",
    "imgs = imgs.cpu().numpy()\n",
    "masks = masks.cpu().numpy()\n",
    "\n",
    "# Displaying some samples\n",
    "num_samples = min(4, len(imgs))  # Displaying 4 images\n",
    "fig, axes = plt.subplots(num_samples, 2, figsize=(8, 4 * num_samples))\n",
    "\n",
    "for i in range(num_samples):\n",
    "    # Converting image from [C, H, W] to [H, W, C] and scale to [0,1] for visualization\n",
    "    img = imgs[i].transpose(1, 2, 0)\n",
    "    # Squeezing the mask to remove the extra dimension\n",
    "    mask = masks[i].squeeze()  # Mask is now [H, W]\n",
    "\n",
    "    # Plotting the original image\n",
    "    axes[i, 0].imshow(img)\n",
    "    axes[i, 0].set_title(\"Image\")\n",
    "    axes[i, 0].axis('off')\n",
    "\n",
    "    # Plotting it's mask\n",
    "    axes[i, 1].imshow(mask, cmap='gray')\n",
    "    axes[i, 1].set_title(\"Mask\")\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GtUU3JpDvj-7"
   },
   "source": [
    "# Model - DeeplabV3 with Resnet101"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeHRxPXTyCIL"
   },
   "source": [
    "## Loading a Pre-trained Deeplabv3 model with Resnet101\n",
    "\n",
    "\n",
    "*   Pretrained on COCO (Common Objects in Context)\n",
    "*   Fine Tuned here on FoodSeg103 - only the last block\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3iWqH7OvYMss",
    "outputId": "85603630-2fe4-4f83-9b32-64e281d96414"
   },
   "outputs": [],
   "source": [
    "# DeepLabV3 with ResNet-101 backbone\n",
    "model = deeplabv3_resnet101(pretrained=True)\n",
    "model.classifier[4] = nn.Conv2d(256, 104, kernel_size=1)  # Adjusting the output layer to have 104 classes including the background\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "# Unfreezing the last ResNet block for fine-tuning\n",
    "for param in model.backbone.parameters(): # This ensures all layers are frozen\n",
    "    param.requires_grad = False\n",
    "for param in model.backbone.layer4.parameters():  # Unfreezing layer4 (last block)\n",
    "    param.requires_grad = True\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() # Using cross entropy loss as the loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4) # Optimiser\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IoW7Nn3fyNLJ"
   },
   "source": [
    "## Calculating Mean IoU to track the model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sb5BGXQ0w4xr"
   },
   "outputs": [],
   "source": [
    "def compute_iou(preds, masks, num_classes=104):\n",
    "    iou_per_class = []\n",
    "\n",
    "    preds = preds.cpu().numpy()\n",
    "    masks = masks.cpu().numpy()\n",
    "\n",
    "    for class_id in range(1, num_classes):  # Excluding the background class\n",
    "        intersection = np.logical_and(preds == class_id, masks == class_id).sum()\n",
    "        union = np.logical_or(preds == class_id, masks == class_id).sum()\n",
    "\n",
    "        if union == 0:\n",
    "            iou = 1.0  # To avoid division by zero\n",
    "        else:\n",
    "            iou = intersection / union\n",
    "\n",
    "        iou_per_class.append(iou)\n",
    "\n",
    "    return np.mean(iou_per_class)  # Mean IoU across all classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1IhIHUrx7Tv"
   },
   "source": [
    "## Model Training  - Uncomment only for training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LjJfIuL9w18C"
   },
   "outputs": [],
   "source": [
    "# # Training loop\n",
    "# epochs = 50\n",
    "# model.train()\n",
    "# for epoch in range(epochs):\n",
    "#     epoch_loss = 0\n",
    "#     miou_scores = []\n",
    "#     for imgs, masks in train_loader:\n",
    "#         imgs, masks = imgs.to(device), masks.to(device)\n",
    "#         optimizer.zero_grad()\n",
    "#         outputs = model(imgs)['out']\n",
    "#         loss = criterion(outputs, masks)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         epoch_loss += loss.item()\n",
    "\n",
    "#         preds = torch.argmax(outputs, dim=1)\n",
    "#         for class_id in range(1, 104):  # Excluding background\n",
    "#             intersection = torch.sum((preds == class_id) & (masks == class_id))\n",
    "#             union = torch.sum((preds == class_id) | (masks == class_id))\n",
    "\n",
    "#             # Print intersection and union values for each class\n",
    "#             # print(f\"Class {class_id}: Intersection = {intersection.item()}, Union = {union.item()}\")\n",
    "\n",
    "#             # Only compute IoU if the class appears in either prediction or ground truth\n",
    "#             if union > 0:\n",
    "#                 iou = (intersection.float() / union.float()).cpu().item()\n",
    "#                 miou_scores.append(iou)\n",
    "\n",
    "#     miou_scores = np.array(miou_scores)\n",
    "#     miou_score = np.mean(miou_scores)\n",
    "#     print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss/len(train_loader):.4f}, Mean IoU: {miou_score:.4f}\")\n",
    "\n",
    "# torch.save(model.state_dict(), \"/content/drive/MyDrive/advanced_ai/task1/task1_food_segmentation_model.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKI0ByIUxhkb"
   },
   "source": [
    "# Model Evaluation using Test Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70riy83zxrGQ"
   },
   "source": [
    "## Loading the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RZUTdf9aU8kn",
    "outputId": "acf725c2-e948-4e66-9144-13159279da52"
   },
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"/content/drive/MyDrive/advanced_ai/task1/task1_food_segmentation_model.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7dUA6ZvxvVm"
   },
   "source": [
    "## Running the model with test image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "id": "lFvmWwVwYMbX",
    "outputId": "daaf4848-5e12-4c3b-fc5d-14611fc1ed9a"
   },
   "outputs": [],
   "source": [
    "model.eval()\n",
    "miou_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, masks in test_loader:\n",
    "        imgs, masks = imgs.to(device), masks.to(device)\n",
    "\n",
    "        # Converting masks to integer class labels\n",
    "        masks = masks.round().long()\n",
    "\n",
    "        outputs = model(imgs)['out']\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "        # Checking unique values in predictions and masks\n",
    "        print(f\"Predicted classes: {torch.unique(preds)}\")\n",
    "        print(f\"Ground truth classes: {torch.unique(masks)}\")\n",
    "\n",
    "        for class_id in range(1, 104):  # Excluding background\n",
    "            intersection = torch.sum((preds == class_id) & (masks == class_id))\n",
    "            union = torch.sum((preds == class_id) | (masks == class_id))\n",
    "\n",
    "            # Print intersection and union values for each class - only uncomment to check if the code is working properly\n",
    "            # print(f\"Class {class_id}: Intersection = {intersection.item()}, Union = {union.item()}\")\n",
    "\n",
    "            # Only compute IoU if the class appears in either prediction or ground truth\n",
    "            if union > 0:\n",
    "                iou = (intersection.float() / union.float()).cpu().item()\n",
    "                miou_scores.append(iou)\n",
    "\n",
    "        # Visualizing original image, ground truth mask, and predicted mask for testing\n",
    "        img = imgs[0].cpu().numpy().transpose(1, 2, 0)  # Converting to HWC format for visualisation\n",
    "        mask = masks[0].squeeze(0).cpu().numpy()  # Ground truth mask\n",
    "        pred_mask = preds[0].cpu().numpy()  # Predicted mask\n",
    "\n",
    "        # Display the images\n",
    "        fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        ax[0].imshow(img)\n",
    "        ax[0].set_title(\"Original Image\")\n",
    "        ax[0].axis('off')\n",
    "        ax[1].imshow(mask, cmap='gray')\n",
    "        ax[1].set_title(\"Ground Truth Mask\")\n",
    "        ax[1].axis('off')\n",
    "        ax[2].imshow(pred_mask, cmap='gray')\n",
    "        ax[2].set_title(\"Predicted Mask\")\n",
    "        ax[2].axis('off')\n",
    "        plt.show()\n",
    "\n",
    "# Computing the mean IoU only over the classes that were present\n",
    "if miou_scores:\n",
    "    miou_score = np.mean(miou_scores)\n",
    "else:\n",
    "    miou_score = 0.0\n",
    "\n",
    "print(f\"Mean IoU: {miou_score:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
