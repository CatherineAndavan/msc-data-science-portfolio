{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mkKqKUFmewaH",
    "outputId": "e82fc607-3bd7-40f3-d4e8-612557537dde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHhiNTlFL2tc"
   },
   "source": [
    "## Required Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OVQZ03LL1Nc",
    "outputId": "aed35b74-63b1-46c6-a525-9b3f840e3035"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "# Required package:\n",
    "import pandas as pd\n",
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Required imports for Tokenization\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Required imports:\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x52RpGX2LtNj"
   },
   "source": [
    "## Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "8B2wu5g5ZY2w"
   },
   "outputs": [],
   "source": [
    "filePath = \"/content/drive/MyDrive/61262-0.txt\"\n",
    "\n",
    "# Read the data using the correct encoding:\n",
    "with open(filePath, \"r\", encoding=\"utf-8\") as data:\n",
    "    text_df = data.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PYfjXjUM1p_"
   },
   "source": [
    "## Preprocessing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dNIIf-lNaFHT",
    "outputId": "558cc04c-ed9a-4126-de33-6e32d61cfca6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3823 sentences in the text\n"
     ]
    }
   ],
   "source": [
    "text_df = re.sub(r\"[^A-Za-z\\s.']\", ' ', text_df)  # Removing characters other than alphabets, space, period, and apostrophe\n",
    "text_df = text_df.lower()  # Converting to lowercase\n",
    "sentences = sent_tokenize(text_df)  # Tokenizing into sentences\n",
    "\n",
    "clean_text = [\n",
    "    [word for word in sentence.split() ] #Splitting sentences into words\n",
    "    for sentence in sentences\n",
    "]\n",
    "\n",
    "print(f'There are {len(clean_text)} sentences in the text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wncypjywqxlb",
    "outputId": "030ed652-8532-495a-e62a-8799d806c024"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The  Training Set includes 50227 sequences of 157 tokens.\n",
      "There are 7005 words in our vocabulary!\n"
     ]
    }
   ],
   "source": [
    "# Initializing Tokenizer with lower = False (keeps the case)\n",
    "tokeniser = Tokenizer(lower=False)\n",
    "\n",
    "# Fitting the Tokeniser to the text in the training data:\n",
    "tokeniser.fit_on_texts(clean_text)\n",
    "\n",
    "# Creating a pickle to store the tokenizer to be later used for testing\n",
    "import pickle\n",
    "with open('tokenizer.pkl', 'wb') as handle:\n",
    "    pickle.dump(tokeniser, handle)\n",
    "\n",
    "# Convert text into sequences of word indices\n",
    "clean_text = tokeniser.texts_to_sequences(clean_text)\n",
    "\n",
    "input_text = [] # -> Stores input sequence\n",
    "output_text = [] # -> Stores corresponding next words\n",
    "\n",
    "for sequence in clean_text:\n",
    "  for i in range(1, len(sequence)):\n",
    "    input_text.append(sequence[:i]) # Input all words upto the current index\n",
    "    output_text.append(sequence[i]) # Output next word\n",
    "\n",
    "# Finding the maximum length to pad all sequences to this length\n",
    "max_length = max(len(sequence) for sequence in input_text)\n",
    "\n",
    "# Converting sequences to the same length using padding and truncating\n",
    "input_text = pad_sequences(input_text, maxlen=max_length, padding='post', truncating='post')\n",
    "\n",
    "# Getting the total number of words, needed for the embedding layer size, +1 is added because of zero padding adding one more index in the vocabulary\n",
    "num_words = len(tokeniser.word_index) + 1\n",
    "\n",
    "print(f'The  Training Set includes {input_text.shape[0]} sequences of {input_text.shape[1]} tokens.')\n",
    "print(f'There are {num_words} words in our vocabulary!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FflFO8NGqxjc",
    "outputId": "85c801b9-1ad8-41a7-c91e-1f692e171f38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Parameters to set\n",
    "embedding_dim = 32 # -> Size of word embedding vector\n",
    "output_dim = 64 # -> Number of LSTM units per layer\n",
    "\n",
    "# Function to create the model:\n",
    "def create_model(embedding_dim, lstm_units, vocab_size, max_length):\n",
    "\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Embedding(num_words, embedding_dim, input_length=max_length)) # -> converting word indices to vectors of fixed size\n",
    "  model.add(layers.LSTM(output_dim, return_sequences= True, dropout= 0.2)) # -> Adding dropouts for regularization\n",
    "  model.add(layers.LSTM(output_dim, return_sequences= True, dropout= 0.3))\n",
    "  model.add(layers.LSTM(output_dim))\n",
    "  model.add(layers.LayerNormalization())\n",
    "  model.add(layers.Dense(num_words, activation='softmax')) # -> using softmax because it is text generation and it requires probability\n",
    "\n",
    "\n",
    "  return model\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "ajB04Hi3qxhA",
    "outputId": "c2221019-377e-4f2b-9db3-bed37af9a085"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer_normalization                  │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LayerNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ layer_normalization                  │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mLayerNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Building the model:\n",
    "model = create_model(embedding_dim, output_dim,num_words, max_length)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "biT9LhGqqxen"
   },
   "outputs": [],
   "source": [
    "# The compiler preparing the model for training:\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "86vj1VU9qP0a",
    "outputId": "360df614-779e-4041-a21c-3064dcef9e84"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 18ms/step - accuracy: 0.0548 - loss: 7.1681\n",
      "Epoch 2/100\n",
      "\u001b[1m  10/1570\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0596 - loss: 6.8578"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/keras/src/callbacks/early_stopping.py:153: UserWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: accuracy,loss\n",
      "  current = self.get_monitor_value(logs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0549 - loss: 6.7746\n",
      "Epoch 3/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0562 - loss: 6.7345\n",
      "Epoch 4/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0557 - loss: 6.7218\n",
      "Epoch 5/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0572 - loss: 6.7232\n",
      "Epoch 6/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0571 - loss: 6.7057\n",
      "Epoch 7/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 18ms/step - accuracy: 0.0561 - loss: 6.7116\n",
      "Epoch 8/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0564 - loss: 6.7004\n",
      "Epoch 9/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0565 - loss: 6.7060\n",
      "Epoch 10/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0564 - loss: 6.7184\n",
      "Epoch 11/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0564 - loss: 6.7117\n",
      "Epoch 12/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0546 - loss: 6.6985\n",
      "Epoch 13/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0552 - loss: 6.7075\n",
      "Epoch 14/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0561 - loss: 6.7194\n",
      "Epoch 15/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0557 - loss: 6.7190\n",
      "Epoch 16/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0554 - loss: 6.7013\n",
      "Epoch 17/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0579 - loss: 6.6946\n",
      "Epoch 18/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0566 - loss: 6.7123\n",
      "Epoch 19/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0568 - loss: 6.6990\n",
      "Epoch 20/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0559 - loss: 6.6961\n",
      "Epoch 21/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0567 - loss: 6.6971\n",
      "Epoch 22/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0563 - loss: 6.7153\n",
      "Epoch 23/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0555 - loss: 6.7081\n",
      "Epoch 24/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0568 - loss: 6.6935\n",
      "Epoch 25/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0560 - loss: 6.6991\n",
      "Epoch 26/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0562 - loss: 6.7002\n",
      "Epoch 27/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0568 - loss: 6.6951\n",
      "Epoch 28/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0549 - loss: 6.7125\n",
      "Epoch 29/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0558 - loss: 6.7035\n",
      "Epoch 30/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0557 - loss: 6.7199\n",
      "Epoch 31/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0568 - loss: 6.7017\n",
      "Epoch 32/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0552 - loss: 6.7137\n",
      "Epoch 33/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0562 - loss: 6.7060\n",
      "Epoch 34/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0539 - loss: 6.6991\n",
      "Epoch 35/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0585 - loss: 6.6950\n",
      "Epoch 36/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0571 - loss: 6.7033\n",
      "Epoch 37/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0556 - loss: 6.7002\n",
      "Epoch 38/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0574 - loss: 6.7020\n",
      "Epoch 39/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0568 - loss: 6.7025\n",
      "Epoch 40/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0566 - loss: 6.7071\n",
      "Epoch 41/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0555 - loss: 6.7049\n",
      "Epoch 42/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0559 - loss: 6.6890\n",
      "Epoch 43/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0568 - loss: 6.6976\n",
      "Epoch 44/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0566 - loss: 6.7188\n",
      "Epoch 45/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0567 - loss: 6.7024\n",
      "Epoch 46/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0544 - loss: 6.7111\n",
      "Epoch 47/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0550 - loss: 6.7031\n",
      "Epoch 48/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0548 - loss: 6.7059\n",
      "Epoch 49/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0552 - loss: 6.7156\n",
      "Epoch 50/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0564 - loss: 6.6834\n",
      "Epoch 51/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0561 - loss: 6.6964\n",
      "Epoch 52/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0568 - loss: 6.7087\n",
      "Epoch 53/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0548 - loss: 6.7165\n",
      "Epoch 54/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0557 - loss: 6.7032\n",
      "Epoch 55/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0565 - loss: 6.6991\n",
      "Epoch 56/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0542 - loss: 6.7089\n",
      "Epoch 57/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0569 - loss: 6.7069\n",
      "Epoch 58/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0559 - loss: 6.6996\n",
      "Epoch 59/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0566 - loss: 6.6740\n",
      "Epoch 60/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0561 - loss: 6.7098\n",
      "Epoch 61/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0589 - loss: 6.7091\n",
      "Epoch 62/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0565 - loss: 6.6970\n",
      "Epoch 63/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0553 - loss: 6.7053\n",
      "Epoch 64/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0571 - loss: 6.6969\n",
      "Epoch 65/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0562 - loss: 6.7072\n",
      "Epoch 66/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0554 - loss: 6.6975\n",
      "Epoch 67/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0552 - loss: 6.7158\n",
      "Epoch 68/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0554 - loss: 6.7060\n",
      "Epoch 69/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0534 - loss: 6.7129\n",
      "Epoch 70/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0558 - loss: 6.7090\n",
      "Epoch 71/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0562 - loss: 6.6695\n",
      "Epoch 72/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0555 - loss: 6.7161\n",
      "Epoch 73/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0573 - loss: 6.6862\n",
      "Epoch 74/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0553 - loss: 6.7021\n",
      "Epoch 75/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0549 - loss: 6.7126\n",
      "Epoch 76/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0553 - loss: 6.7073\n",
      "Epoch 77/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0554 - loss: 6.6942\n",
      "Epoch 78/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0565 - loss: 6.6903\n",
      "Epoch 79/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0546 - loss: 6.7116\n",
      "Epoch 80/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0576 - loss: 6.6971\n",
      "Epoch 81/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0555 - loss: 6.6941\n",
      "Epoch 82/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0563 - loss: 6.7069\n",
      "Epoch 83/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0566 - loss: 6.6935\n",
      "Epoch 84/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0578 - loss: 6.7034\n",
      "Epoch 85/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0560 - loss: 6.6983\n",
      "Epoch 86/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0545 - loss: 6.7338\n",
      "Epoch 87/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0559 - loss: 6.7054\n",
      "Epoch 88/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0538 - loss: 6.7168\n",
      "Epoch 89/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0549 - loss: 6.6988\n",
      "Epoch 90/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0558 - loss: 6.6925\n",
      "Epoch 91/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0568 - loss: 6.7047\n",
      "Epoch 92/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0563 - loss: 6.7177\n",
      "Epoch 93/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0574 - loss: 6.7001\n",
      "Epoch 94/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0557 - loss: 6.7095\n",
      "Epoch 95/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0552 - loss: 6.7015\n",
      "Epoch 96/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0564 - loss: 6.7004\n",
      "Epoch 97/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0552 - loss: 6.6961\n",
      "Epoch 98/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0562 - loss: 6.7148\n",
      "Epoch 99/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0553 - loss: 6.7187\n",
      "Epoch 100/100\n",
      "\u001b[1m1570/1570\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 17ms/step - accuracy: 0.0563 - loss: 6.7027\n"
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "output_text = np.array(output_text)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit(input_text, output_text, epochs=epochs, batch_size=batch_size, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "# Save trained model\n",
    "model.save('/content/drive/MyDrive/deep_learning/lstm_model_text_generator.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5MyyjEdKqPxE"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Loading the trained model\n",
    "model = load_model('/content/drive/MyDrive/deep_learning/lstm_model_text_generator.keras')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "h_XKVY3ZqPo8"
   },
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, seed_text, max_length, num_words=50, temperature=1.0 ):\n",
    "    for _ in range(num_words):\n",
    "        # Converting seed text to sequence\n",
    "        sequence = tokenizer.texts_to_sequences([seed_text])\n",
    "        sequence = pad_sequences(sequence, maxlen=max_length, padding='pre')\n",
    "\n",
    "        # Predicting next word probabilities\n",
    "        predicted_probs = model.predict(sequence, verbose=0)[0]\n",
    "\n",
    "        # Applying temperature scaling\n",
    "        predicted_probs = np.asarray(predicted_probs).astype(\"float64\")\n",
    "        predicted_probs = np.log(predicted_probs + 1e-9) / temperature  # Avoiding log(0)\n",
    "        exp_preds = np.exp(predicted_probs)\n",
    "        predicted_probs = exp_preds / np.sum(exp_preds)\n",
    "\n",
    "        # Sampling from the probability distribution\n",
    "        predicted_index = np.random.choice(len(predicted_probs), p=predicted_probs)\n",
    "\n",
    "        # Converting index to word\n",
    "        predicted_word = tokenizer.index_word.get(predicted_index, \"\")\n",
    "\n",
    "        # To ensure predicted word is a string\n",
    "        if not isinstance(predicted_word, str):\n",
    "            continue  # Skip if it's not a valid string\n",
    "\n",
    "        # Skipping numbers or invalid words\n",
    "        if not predicted_word.isalpha():\n",
    "            continue\n",
    "\n",
    "        # Appending the predicted word\n",
    "        seed_text += \" \" + predicted_word\n",
    "\n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rVm-mUkH8qKY"
   },
   "outputs": [],
   "source": [
    "# Loading the tokenizer\n",
    "\n",
    "with open('tokenizer.pkl', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "K20ufgemqPla",
    "outputId": "3d1f2237-662d-4146-eaba-20e488c90afc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There was a murder in the building point about your not too that s was temporary sold heeds of poirot i packet our look was drawer i or the which fianc influence concerning street the my table house match number from of hastings out very arm do to that seen the sad but fibre\n"
     ]
    }
   ],
   "source": [
    "# Define a seed text\n",
    "text_prompt = \"There was a murder in the building\"\n",
    "\n",
    "# Generate text\n",
    "generated_text = generate_text(model, tokenizer, text_prompt, max_length=100)\n",
    "\n",
    "# Print generated text\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
